{"cells":[{"cell_type":"markdown","source":["Lab 3:\n","- Carlos Jarrin\n","- Fausto Yugcha"],"metadata":{"id":"HEij2UBdffzM"},"id":"HEij2UBdffzM"},{"cell_type":"markdown","id":"e2c88e42-50d4-4b4d-bdf1-894bcff1a41d","metadata":{"id":"e2c88e42-50d4-4b4d-bdf1-894bcff1a41d"},"source":["# NLP and Neural Networks\n","\n","In this exercise, we'll apply our knowledge of neural networks to process natural language. As we did in the bigram exercise, the goal of this lab is to predict the next word, given the previous one."]},{"cell_type":"markdown","id":"5132d376-54d7-48c3-a52c-ac3d94ed798b","metadata":{"id":"5132d376-54d7-48c3-a52c-ac3d94ed798b"},"source":["### Data set\n","\n","Load the text from \"One Hundred Years of Solitude\" that we used in our bigrams exercise. It's located in the data folder."]},{"cell_type":"code","execution_count":1,"id":"VjJEjVhGH9WI","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22959,"status":"ok","timestamp":1725510995461,"user":{"displayName":"fgony","userId":"12699453911390145003"},"user_tz":300},"id":"VjJEjVhGH9WI","outputId":"f74e21a6-0d30-40ae-f6fe-66c0a6afe3bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","id":"9e309d79-7746-40e3-8a02-3cc7b45c16ac","metadata":{"id":"9e309d79-7746-40e3-8a02-3cc7b45c16ac"},"source":["### Important note:\n","\n","Start with a smaller part of the text. Maybe the first 10 parragraphs, as the number of tokens rapidly increases as we add more text.\n","\n","Later you can use a bigger corpus."]},{"cell_type":"markdown","id":"3d1393d6-7cbf-4e8e-a699-0f0cd28982a3","metadata":{"id":"3d1393d6-7cbf-4e8e-a699-0f0cd28982a3"},"source":["Don't forget to prepare the data by generating the corresponding tokens."]},{"cell_type":"code","execution_count":2,"id":"9bbced32-a252-48b0-bc8f-cecfdcf1ec2e","metadata":{"id":"9bbced32-a252-48b0-bc8f-cecfdcf1ec2e","executionInfo":{"status":"ok","timestamp":1725511003568,"user_tz":300,"elapsed":5418,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from nltk import bigrams\n","from nltk.tokenize import TreebankWordTokenizer\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score\n","\n","import numpy as np"]},{"cell_type":"code","execution_count":3,"id":"bmeYOB2ZIxUy","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1576,"status":"ok","timestamp":1725511020879,"user":{"displayName":"fgony","userId":"12699453911390145003"},"user_tz":300},"id":"bmeYOB2ZIxUy","outputId":"cbe1618d-55bc-4a99-f687-e72c75ca9693"},"outputs":[{"output_type":"stream","name":"stdout","text":["tokens = len(tokens)=6293\n"]}],"source":["# Cargar el texto y tokenizar\n","tokenizer = TreebankWordTokenizer()\n","text = open('/content/drive/MyDrive/NLP/datos/cap1.txt', 'r').read().lower()\n","tokens = tokenizer.tokenize(text)\n","print(f\"tokens = {len(tokens)=}\")"]},{"cell_type":"markdown","id":"7681843a-18f0-4d7c-9b02-83015f4383e1","metadata":{"id":"7681843a-18f0-4d7c-9b02-83015f4383e1"},"source":["### Let's prepare the data set.\n","\n","Our neural network needs to have an input X and an output y. Remember that these sets are numerical, so you'd need something to map the tokens into numbers, and viceversa."]},{"cell_type":"code","execution_count":4,"id":"29c10640-f146-478a-a1a4-d2e747af5ea6","metadata":{"id":"29c10640-f146-478a-a1a4-d2e747af5ea6","executionInfo":{"status":"ok","timestamp":1725511045277,"user_tz":300,"elapsed":699,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"outputs":[],"source":["# Generar bigramas (pares de palabras)\n","bigram_list = list(bigrams(tokens))"]},{"cell_type":"code","execution_count":5,"id":"FxjSpRfGJw34","metadata":{"id":"FxjSpRfGJw34","executionInfo":{"status":"ok","timestamp":1725511047309,"user_tz":300,"elapsed":332,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"outputs":[],"source":["X = [bigram[0] for bigram in bigram_list] # Primera palabra del bigrama\n","y = [bigram[1] for bigram in bigram_list] # Segunda palabra del bigrama"]},{"cell_type":"code","execution_count":6,"id":"0PdcrSf2SYuh","metadata":{"id":"0PdcrSf2SYuh","executionInfo":{"status":"ok","timestamp":1725511049845,"user_tz":300,"elapsed":629,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"outputs":[],"source":["# Convertir las palabras a una representación numérica\n","vectorizer = CountVectorizer()\n","X_vectorized = vectorizer.fit_transform(X)"]},{"cell_type":"code","execution_count":7,"id":"6YhzKGMnVKpl","metadata":{"id":"6YhzKGMnVKpl","executionInfo":{"status":"ok","timestamp":1725511051950,"user_tz":300,"elapsed":1,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"outputs":[],"source":["# Dividir los datos en entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size = 0.2, random_state = 0)"]},{"cell_type":"code","execution_count":8,"id":"yIJBTpu9QaZR","metadata":{"id":"yIJBTpu9QaZR","executionInfo":{"status":"ok","timestamp":1725511053919,"user_tz":300,"elapsed":1,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"outputs":[],"source":["# Codificar las etiquetas\n","label_encoder = LabelEncoder()\n","y_encoded = label_encoder.fit_transform(y)\n","\n","# Convertir los datos a tensores\n","X_tensor = torch.tensor(X_vectorized.toarray(), dtype = torch.float32)\n","y_tensor = torch.tensor(y_encoded, dtype=torch.long)\n","\n","# Dividir en conjunto de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size = 0.2, random_state = 0)"]},{"cell_type":"code","execution_count":null,"id":"5984fd00-bdbf-4403-a341-b7ef83138db2","metadata":{"id":"5984fd00-bdbf-4403-a341-b7ef83138db2"},"outputs":[],"source":["# Note that our vectors are integers, which can be thought as a categorical variables.\n","# torch provides the one_hot method, that would generate tensors suitable for our nn\n","# make sure that the dtype of your tensor is float."]},{"cell_type":"code","source":["type(X_tensor)\n","type(y_tensor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bJBJV2zCONWZ","executionInfo":{"status":"ok","timestamp":1725511083579,"user_tz":300,"elapsed":683,"user":{"displayName":"fgony","userId":"12699453911390145003"}},"outputId":"66da31be-51b1-4d3d-c1ae-abc044edf698"},"id":"bJBJV2zCONWZ","execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Tensor"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","id":"cda25114-c6ae-4e07-a743-12e10cd77796","metadata":{"id":"cda25114-c6ae-4e07-a743-12e10cd77796"},"source":["### Network design\n","To start, we are going to have a very simple network. Define a single layer network"]},{"cell_type":"code","execution_count":11,"id":"Qx0BrDKOaAo6","metadata":{"id":"Qx0BrDKOaAo6","executionInfo":{"status":"ok","timestamp":1725511087842,"user_tz":300,"elapsed":327,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"outputs":[],"source":["# Parámetros de la red\n","input_size = X_train.shape[1]\n","hidden_size = 128  # Ajustado para más capas ocultas\n","output_size = len(label_encoder.classes_)\n","dropout_rate = 0.3  # Para evitar el sobreajuste"]},{"cell_type":"code","execution_count":12,"id":"99c50d1e-3842-451e-8ede-1c68dddf3843","metadata":{"id":"99c50d1e-3842-451e-8ede-1c68dddf3843","executionInfo":{"status":"ok","timestamp":1725511094047,"user_tz":300,"elapsed":677,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"outputs":[],"source":["# Definir una red neuronal más profunda\n","class ImprovedNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(ImprovedNN, self).__init__()\n","        # Primera capa densa\n","        self.fc1 = nn.Linear(input_size, hidden_size)\n","        self.relu1 = nn.ReLU()\n","        # Segunda capa densa\n","        self.fc2 = nn.Linear(hidden_size, hidden_size)\n","        self.relu2 = nn.ReLU()\n","        # Dropout para evitar sobreajuste\n","        self.dropout = nn.Dropout(dropout_rate)\n","        # Capa de salida\n","        self.fc3 = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.relu1(x)\n","        x = self.fc2(x)\n","        x = self.relu2(x)\n","        x = self.dropout(x)\n","        x = self.fc3(x)\n","        return x"]},{"cell_type":"code","execution_count":13,"id":"b62f0fdc-bfb8-4081-acfa-bcab5226b82e","metadata":{"id":"b62f0fdc-bfb8-4081-acfa-bcab5226b82e","executionInfo":{"status":"ok","timestamp":1725511099151,"user_tz":300,"elapsed":1661,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"outputs":[],"source":["# Crear el modelo\n","model = ImprovedNN(input_size, hidden_size, output_size)\n","\n","# Definir el criterio de pérdida y el optimizador\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":14,"id":"eb8b0c9b-55b2-4397-8a0d-e4b13fd2fbec","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eb8b0c9b-55b2-4397-8a0d-e4b13fd2fbec","outputId":"19a40c8b-325a-4f58-e263-1df9a763828e","executionInfo":{"status":"ok","timestamp":1725511167286,"user_tz":300,"elapsed":65805,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [20/180], Loss: 7.5192, Train Accuracy: 1.63%, Test Accuracy: 0.08%\n","Epoch [40/180], Loss: 6.6586, Train Accuracy: 4.61%, Test Accuracy: 6.75%\n","Epoch [60/180], Loss: 5.7641, Train Accuracy: 6.56%, Test Accuracy: 6.75%\n","Epoch [80/180], Loss: 5.5892, Train Accuracy: 7.59%, Test Accuracy: 6.91%\n","Epoch [100/180], Loss: 5.4992, Train Accuracy: 7.53%, Test Accuracy: 7.31%\n","Epoch [120/180], Loss: 5.4224, Train Accuracy: 8.88%, Test Accuracy: 7.07%\n","Epoch [140/180], Loss: 5.3007, Train Accuracy: 12.30%, Test Accuracy: 8.90%\n","Epoch [160/180], Loss: 5.1183, Train Accuracy: 16.49%, Test Accuracy: 9.13%\n","Epoch [180/180], Loss: 4.8893, Train Accuracy: 19.99%, Test Accuracy: 8.98%\n"]}],"source":["# Entrenar el modelo\n","n_epochs = 180\n","\n","for epoch in range(n_epochs):\n","    model.train()\n","\n","    # Hacer predicciones y calcular la pérdida\n","    outputs = model(X_train)\n","    loss = criterion(outputs, y_train)\n","\n","    # Actualizar los pesos\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    # Calcular precisión en los datos de entrenamiento\n","    _, predicted = torch.max(outputs, 1)\n","    train_accuracy = accuracy_score(y_train, predicted)\n","\n","    # Evaluar en el conjunto de prueba\n","    model.eval()\n","    with torch.no_grad():\n","        outputs_test = model(X_test)\n","        _, predicted_test = torch.max(outputs_test, 1)\n","        test_accuracy = accuracy_score(y_test, predicted_test)\n","\n","    if (epoch+1) % 20 == 0:\n","        print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}, Train Accuracy: {train_accuracy*100:.2f}%, Test Accuracy: {test_accuracy*100:.2f}%\")\n"]},{"cell_type":"code","execution_count":15,"id":"vgZF-xh23sd2","metadata":{"id":"vgZF-xh23sd2","executionInfo":{"status":"ok","timestamp":1725511187651,"user_tz":300,"elapsed":496,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"outputs":[],"source":["# Función para predecir la siguiente palabra dada una palabra\n","def predict_next_word(input_word):\n","    input_vectorized = vectorizer.transform([input_word]).toarray()\n","    input_tensor = torch.tensor(input_vectorized, dtype=torch.float32)\n","\n","    model.eval()\n","    with torch.no_grad():\n","        output = model(input_tensor)\n","        probabilities = torch.softmax(output, dim=1)\n","        predicted_prob, predicted_idx = torch.max(probabilities, 1)\n","\n","    predicted_word = label_encoder.inverse_transform(predicted_idx.numpy())[0]\n","    return predicted_word, predicted_prob.item()"]},{"cell_type":"code","execution_count":18,"id":"1ac920e4-e9fc-4760-b031-9c959d78bae5","metadata":{"id":"1ac920e4-e9fc-4760-b031-9c959d78bae5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725511235011,"user_tz":300,"elapsed":645,"user":{"displayName":"fgony","userId":"12699453911390145003"}},"outputId":"c9840686-1184-4be3-ece9-f81c5bcc035b"},"outputs":[{"output_type":"stream","name":"stdout","text":["aldea , de la la la la la la la la\n","aldea , de la la la la la la la la\n","aldea , de la la la la la la la la\n","aldea , de la la la la la la la la\n","aldea , de la la la la la la la la\n","aldea , de la la la la la la la la\n","aldea , de la la la la la la la la\n","aldea , de la la la la la la la la\n","aldea , de la la la la la la la la\n","aldea , de la la la la la la la la\n"]}],"source":["# Probar predicción\n","max_n_pred = 10\n","for _ in range(10):\n","  word = 'aldea'\n","  full_pred = word\n","  for i in range(max_n_pred):\n","    word2 = predict_next_word(word)[0]\n","    full_pred = full_pred + ' ' + word2\n","    word = word2\n","  print(full_pred)"]},{"cell_type":"markdown","id":"9e2d09aa-8a47-4668-b1b1-5080be8851ed","metadata":{"id":"9e2d09aa-8a47-4668-b1b1-5080be8851ed"},"source":["### Analysis"]},{"cell_type":"markdown","id":"2d06d9c5-4df1-4145-812f-ff86958154c1","metadata":{"id":"2d06d9c5-4df1-4145-812f-ff86958154c1"},"source":["1. Test your network with a few words"]},{"cell_type":"code","source":["def pred_n_words(word = 'buendia', max_n_pred = 10):\n","  full_pred = word\n","  l1 = 0\n","  for i in range(max_n_pred):\n","    word2 = predict_next_word(word)[0]\n","    pr = predict_next_word(word)[1]\n","    full_pred = full_pred + ' ' + word2\n","    word = word2\n","    l1 += np.log(pr)\n","\n","  n_ll = l1/max_n_pred\n","  print(full_pred, '| neg log:', n_ll)\n","\n","palabras = ['buendia', 'niño', 'posibilidad', 'casa', 'muchos']\n","for w in palabras:\n","  pred_n_words(word = w, max_n_pred =1)\n","print(' ')\n","for w in palabras:\n","  pred_n_words(word = w)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GsVoiWPI-zEY","executionInfo":{"status":"ok","timestamp":1725511266922,"user_tz":300,"elapsed":602,"user":{"displayName":"fgony","userId":"12699453911390145003"}},"outputId":"9058b8c7-5e44-4710-d2f1-18ebc2194a56"},"id":"GsVoiWPI-zEY","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["buendia de | neg log: -3.258164787668704\n","niño de | neg log: -1.6824598039681637\n","posibilidad de | neg log: -0.16344186687913714\n","casa , | neg log: -1.998403876553244\n","muchos , | neg log: -3.0167437219622677\n"," \n","buendia de la la la la la la la la la | neg log: -4.937202498653687\n","niño de la la la la la la la la la | neg log: -4.779632000283633\n","posibilidad de la la la la la la la la la | neg log: -4.62773020657473\n","casa , de la la la la la la la la | neg log: -4.588454281295556\n","muchos , de la la la la la la la la | neg log: -4.690288265836459\n"]}]},{"cell_type":"markdown","source":["2. What does each value in the tensor represents?\n","\n","Al ser un tensor de convolucion requiere de valores en forma matricial para funcionar de manera adecuada, por lo que el tensor proporcionado debe ajustarse."],"metadata":{"id":"UpI7BzmRVMHd"},"id":"UpI7BzmRVMHd"},{"cell_type":"markdown","id":"2d0e838c-3cab-4b6f-b041-1fde6a6d29aa","metadata":{"id":"2d0e838c-3cab-4b6f-b041-1fde6a6d29aa"},"source":["\n","3. Why does it make sense to choose that number of neurons in our layer?\n","\n","\n","Cada capa de entrada debe tener la misma cantidad de salida por que asi fue definido el biagram."]},{"cell_type":"markdown","source":["4. What's the negative likelihood for each example?\n","\n","Es una medida que nos ayuda a cuantificar segun el modelo propuesto que tan probable es que la palabra sea la verdadera."],"metadata":{"id":"J8GpiQPmVQQt"},"id":"J8GpiQPmVQQt"},{"cell_type":"markdown","source":["5. Try generating a few sentences?\n","\n","Se debe generar con un bucle para generar un amplio vocabulario y no repetir las mismas palabras en bucle cerrado.\n"],"metadata":{"id":"Y4WEgNGrVEnt"},"id":"Y4WEgNGrVEnt"},{"cell_type":"markdown","source":["6. What's the negative likelihood for each sentence?\n","\n","Vendria a ser la sumatoria de cada una de las palabras."],"metadata":{"id":"k5wfs-PaOfmx"},"id":"k5wfs-PaOfmx"}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}