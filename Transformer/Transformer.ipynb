{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO3XPyEiHGrGCc/zX0GU9l4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"684253afdcf7474b8733f199a480fe39":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0d6563c8e9c14d00a03be52d20cc6fc0","IPY_MODEL_c96db3612100410494761125f140ec42","IPY_MODEL_1d3d1e374f104bc0843c576447740654"],"layout":"IPY_MODEL_f09fa39cb5e44fe7903fd31217a2f078"}},"0d6563c8e9c14d00a03be52d20cc6fc0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfd8615503bb47709420aec14de7ecb9","placeholder":"​","style":"IPY_MODEL_3a5d160abe894998b14dd8c0279c5e4f","value":"Epoch 29: 100%"}},"c96db3612100410494761125f140ec42":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_92ea2ce457584bb6b0eea146c814be5f","max":1870,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b95564e9186248f7b5dad5d9a583d70e","value":1870}},"1d3d1e374f104bc0843c576447740654":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0536b75a440a482c8630177850ac6b14","placeholder":"​","style":"IPY_MODEL_5419785caa584018a69abe292a66ac42","value":" 1870/1870 [00:24&lt;00:00, 76.93it/s, v_num=1]"}},"f09fa39cb5e44fe7903fd31217a2f078":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"dfd8615503bb47709420aec14de7ecb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a5d160abe894998b14dd8c0279c5e4f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"92ea2ce457584bb6b0eea146c814be5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b95564e9186248f7b5dad5d9a583d70e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0536b75a440a482c8630177850ac6b14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5419785caa584018a69abe292a66ac42":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["Lab 6\n","- Carlos Jarrin\n","- Fausto Yugcha"],"metadata":{"id":"8JNB7ufWQuO1"}},{"cell_type":"code","source":["# PyTorch Lightning\n","!pip install --quiet pytorch-lightning>=1.4"],"metadata":{"id":"B9OjuWcdUPXA","executionInfo":{"status":"ok","timestamp":1726018244991,"user_tz":300,"elapsed":3688,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":102,"outputs":[]},{"cell_type":"code","source":["#Librerias\n","import unicodedata\n","import re\n","import math\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","## PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data as data\n","\n","# PyTorch Lightning\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping"],"metadata":{"id":"tbI7b3OoQt_z","executionInfo":{"status":"ok","timestamp":1726014999474,"user_tz":300,"elapsed":2,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### Data"],"metadata":{"id":"tkQh95i5RBMg"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s5IAsZxlQt87","executionInfo":{"status":"ok","timestamp":1726016874343,"user_tz":300,"elapsed":1612,"user":{"displayName":"fgony","userId":"12699453911390145003"}},"outputId":"69c286ae-194c-44ab-bd51-a407d535d999"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["file = '/content/drive/MyDrive/NLP/datos/spa.txt'"],"metadata":{"id":"4fGyk24IQt6c","executionInfo":{"status":"ok","timestamp":1726015023062,"user_tz":300,"elapsed":3,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["SOS_token = 0\n","EOS_token = 1\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","        self.n_words = 2  # Count SOS and EOS\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1"],"metadata":{"id":"N623n1L5Qt4G","executionInfo":{"status":"ok","timestamp":1726015023062,"user_tz":300,"elapsed":2,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def unicodeToAscii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","\n","# Lowercase, trim, and remove non-letter characters\n","def normalizeString(s):\n","    s = unicodeToAscii(s.lower().strip())\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n","    return s.strip()"],"metadata":{"id":"OwOoYtDYRLtP","executionInfo":{"status":"ok","timestamp":1726015023062,"user_tz":300,"elapsed":2,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["MAX_LENGTH = 10\n","\n","eng_prefixes = (\n","    \"i am \", \"i m \",\n","    \"he is\", \"he s \",\n","    \"she is\", \"she s \",\n","    \"you are\", \"you re \",\n","    \"we are\", \"we re \",\n","    \"they are\", \"they re \"\n",")\n","\n","def filterPair(p):\n","    try:\n","        return len(p[0].split(' ')) < MAX_LENGTH and \\\n","            len(p[1].split(' ')) < MAX_LENGTH #and \\\n","#            p[0].startswith(eng_prefixes)\n","    except:\n","        print(p)\n","\n","\n","def filterPairs(pairs):\n","    return [pair for pair in pairs if filterPair(pair)]"],"metadata":{"id":"ioCdZMRhRCKb","executionInfo":{"status":"ok","timestamp":1726015023062,"user_tz":300,"elapsed":2,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def prepareData(lang1, lang2, file):\n","    text = open(file, encoding='utf-8').read().split('\\n')\n","    pairs = [[normalizeString(s) for s in l.split('\\t')][:2] for l in text ]\n","    pairs = [pair for pair in pairs if len(pair) == 2]\n","\n","    input_lang = Lang(lang1)\n","    output_lang = Lang(lang2)\n","\n","    pairs = filterPairs(pairs)\n","\n","    for pair in pairs:\n","        input_lang.addSentence(pair[0])\n","        output_lang.addSentence(pair[1])\n","    print(\"Counted words:\")\n","    print(input_lang.name, input_lang.n_words)\n","    print(output_lang.name, output_lang.n_words)\n","    return input_lang, output_lang, pairs"],"metadata":{"id":"v7sNPvl4Qt1j","executionInfo":{"status":"ok","timestamp":1726015023062,"user_tz":300,"elapsed":2,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["input_lang, output_lang, pairs = prepareData('eng', 'spa', file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_y5i30ZWQtzM","executionInfo":{"status":"ok","timestamp":1726015037979,"user_tz":300,"elapsed":14919,"user":{"displayName":"fgony","userId":"12699453911390145003"}},"outputId":"2151c06d-e51e-4f70-f690-8f2cbf684819"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Counted words:\n","eng 12105\n","spa 23411\n"]}]},{"cell_type":"code","source":["class TranslationDataset(torch.utils.data.Dataset):\n","    def __init__(self, pairs, input_lang, output_lang, max_length):\n","        self.pairs = pairs\n","        self.input_lang = input_lang\n","        self.output_lang = output_lang\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.pairs)\n","\n","    def __getitem__(self, index):\n","        input_sentence = self.pairs[index][0]\n","        output_sentence = self.pairs[index][1]\n","        input_tensor = self.tensor_from_sentence(self.input_lang, input_sentence)\n","        output_tensor = self.tensor_from_sentence(self.output_lang, output_sentence)\n","        return input_tensor, output_tensor\n","\n","    def tensor_from_sentence(self, lang, sentence):\n","        indices = [lang.word2index[word] for word in sentence.split(' ')]\n","        indices.append(EOS_token)\n","        return torch.tensor(indices, dtype=torch.long)"],"metadata":{"id":"xqoPbDslQtwk","executionInfo":{"status":"ok","timestamp":1726015037979,"user_tz":300,"elapsed":5,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def collate_fn(batch):\n","    input_seqs, target_seqs = zip(*batch)\n","\n","    # Find the maximum length of input and target sequences in this batch\n","    input_max_length = max(len(s) for s in input_seqs)\n","    target_max_length = max(len(s) for s in target_seqs)\n","\n","    # Find the overall maximum length for both input and target sequences\n","    max_length = max(input_max_length, target_max_length)\n","\n","    # Pad input sequences to the same maximum length\n","    padded_input_seqs = [torch.cat((s, torch.zeros(max_length - len(s), dtype=torch.long))) for s in input_seqs]\n","\n","    # Pad target sequences to the same maximum length\n","    padded_target_seqs = [torch.cat((s, torch.zeros(max_length - len(s), dtype=torch.long))) for s in target_seqs]\n","\n","    # Stack into tensors\n","    input_tensor = torch.stack(padded_input_seqs)\n","    target_tensor = torch.stack(padded_target_seqs)\n","\n","    return input_tensor, target_tensor"],"metadata":{"id":"21n-Vg2yRaga","executionInfo":{"status":"ok","timestamp":1726015037979,"user_tz":300,"elapsed":5,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def scaled_dot_product(q, k, v, mask=None):\n","    d_k = q.size()[-1]\n","    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n","    attn_logits = attn_logits / math.sqrt(d_k)\n","    if mask is not None:\n","        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n","    attention = F.softmax(attn_logits, dim=-1)\n","    values = torch.matmul(attention, v)\n","    return values, attention"],"metadata":{"id":"7YGtzY4aTG-D","executionInfo":{"status":"ok","timestamp":1726015037979,"user_tz":300,"elapsed":4,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Helper function to support different mask shapes.\n","# Output shape supports (batch_size, number of heads, seq length, seq length)\n","# If 2D: broadcasted over batch size and number of heads\n","# If 3D: broadcasted over number of heads\n","# If 4D: leave as is\n","def expand_mask(mask):\n","    assert mask.ndim >= 2, \"Mask must be at least 2-dimensional with seq_length x seq_length\"\n","    if mask.ndim == 3:\n","        mask = mask.unsqueeze(1)\n","    while mask.ndim < 4:\n","        mask = mask.unsqueeze(0)\n","    return mask"],"metadata":{"id":"sNUJAYHNRaeH","executionInfo":{"status":"ok","timestamp":1726015037979,"user_tz":300,"elapsed":4,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["### Red"],"metadata":{"id":"TRRj_52FThI6"}},{"cell_type":"code","source":["# MultiheadAttention module\n","class MultiheadAttention(nn.Module):\n","    def __init__(self, input_dim, embed_dim, num_heads):\n","        super().__init__()\n","        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n","\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","\n","        # Stack all weight matrices 1...h together for efficiency\n","        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n","        self.o_proj = nn.Linear(embed_dim, embed_dim)\n","        self._reset_parameters()\n","\n","    def _reset_parameters(self):\n","        nn.init.xavier_uniform_(self.qkv_proj.weight)\n","        self.qkv_proj.bias.data.fill_(0)\n","        nn.init.xavier_uniform_(self.o_proj.weight)\n","        self.o_proj.bias.data.fill_(0)\n","\n","    def forward(self, x, mask=None, return_attention=False):\n","        batch_size, seq_length, _ = x.size()\n","        if mask is not None:\n","            mask = expand_mask(mask)\n","        qkv = self.qkv_proj(x)\n","        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n","        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n","        q, k, v = qkv.chunk(3, dim=-1)\n","\n","        values, attention = scaled_dot_product(q, k, v, mask=mask)\n","        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n","        values = values.reshape(batch_size, seq_length, self.embed_dim)\n","        o = self.o_proj(values)\n","\n","        if return_attention:\n","            return o, attention\n","        else:\n","            return o"],"metadata":{"id":"Vl6G0sfmRabv","executionInfo":{"status":"ok","timestamp":1726015037979,"user_tz":300,"elapsed":4,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Transformer Encoder Block\n","class EncoderBlock(nn.Module):\n","    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n","        super().__init__()\n","        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n","        self.linear_net = nn.Sequential(\n","            nn.Linear(input_dim, dim_feedforward),\n","            nn.Dropout(dropout),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(dim_feedforward, input_dim)\n","        )\n","        self.norm1 = nn.LayerNorm(input_dim)\n","        self.norm2 = nn.LayerNorm(input_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask=None):\n","        attn_out = self.self_attn(x, mask=mask)\n","        x = x + self.dropout(attn_out)\n","        x = self.norm1(x)\n","        linear_out = self.linear_net(x)\n","        x = x + self.dropout(linear_out)\n","        x = self.norm2(x)\n","        return x"],"metadata":{"id":"SLS-IZU_RaZf","executionInfo":{"status":"ok","timestamp":1726015037979,"user_tz":300,"elapsed":4,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Transformer Encoder\n","class TransformerEncoder(nn.Module):\n","    def __init__(self, num_layers, **block_args):\n","        super().__init__()\n","        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n","\n","    def forward(self, x, mask=None):\n","        for l in self.layers:\n","            x = l(x, mask=mask)\n","        return x"],"metadata":{"id":"d5S3eXu5RaW3","executionInfo":{"status":"ok","timestamp":1726015037979,"user_tz":300,"elapsed":4,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Positional Encoding\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe, persistent=False)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1)]\n","        return x\n"],"metadata":{"id":"6aLMmLm3RaSf","executionInfo":{"status":"ok","timestamp":1726015037979,"user_tz":300,"elapsed":3,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Transformer model class\n","class TransformerPredictor(pl.LightningModule):\n","    def __init__(self, vocab_size, model_dim, num_classes, num_heads, num_layers, lr, warmup, max_iters, dropout=0.0, input_dropout=0.0):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self._create_model()\n","\n","    def _create_model(self):\n","        # Usar una capa de embedding en lugar de la capa lineal\n","        self.embedding = nn.Embedding(self.hparams.vocab_size, self.hparams.model_dim)\n","        self.positional_encoding = PositionalEncoding(d_model=self.hparams.model_dim)\n","        self.transformer = TransformerEncoder(num_layers=self.hparams.num_layers,\n","                                              input_dim=self.hparams.model_dim,\n","                                              dim_feedforward=2*self.hparams.model_dim,\n","                                              num_heads=self.hparams.num_heads,\n","                                              dropout=self.hparams.dropout)\n","        self.output_net = nn.Sequential(\n","            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n","            nn.LayerNorm(self.hparams.model_dim),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(self.hparams.dropout),\n","            nn.Linear(self.hparams.model_dim, self.hparams.num_classes)\n","        )\n","\n","    def forward(self, x, mask=None, add_positional_encoding=True):\n","        # Aplicar la capa de embeddings en la secuencia de entrada\n","        x = self.embedding(x)\n","        if add_positional_encoding:\n","            x = self.positional_encoding(x)\n","        x = self.transformer(x, mask=mask)\n","        x = self.output_net(x)\n","        return x\n","\n","    @torch.no_grad()\n","    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n","        x = self.input_net(x)\n","        if add_positional_encoding:\n","            x = self.positional_encoding(x)\n","        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n","        return attention_maps\n","\n","    def configure_optimizers(self):\n","        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr, betas=(0.9, 0.98), eps=1e-9)\n","        return optimizer\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        #print(\"Input shape:\", x.shape, \"Target shape:\", y.shape)\n","        mask = (x != 0).unsqueeze(1).unsqueeze(2)\n","        y_hat = self(x, mask)\n","        #print(\"Output shape:\", y_hat.shape)\n","        loss = F.cross_entropy(y_hat.view(-1, y_hat.size(-1)), y.view(-1),ignore_index=0)\n","        self.log('train_loss', loss)\n","        return loss"],"metadata":{"id":"LR3Q74J7RaP_","executionInfo":{"status":"ok","timestamp":1726015037979,"user_tz":300,"elapsed":3,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","vocab_size = max(input_lang.n_words, output_lang.n_words)\n","model_dim = 32\n","num_classes = vocab_size\n","num_heads = 2\n","num_layers = 2\n","lr = 1e-3\n","warmup = 5000\n","max_iters = 100\n","dropout = 0.1\n","input_dropout = 0.1\n","batch_size = 64\n","num_epochs = 30"],"metadata":{"id":"50in8l0VRaNv","executionInfo":{"status":"ok","timestamp":1726016210566,"user_tz":300,"elapsed":401,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"],"metadata":{"id":"oJJ8FIOUVGkI","executionInfo":{"status":"ok","timestamp":1726016214568,"user_tz":300,"elapsed":468,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["# Crear dataset\n","train_dataset = TranslationDataset(pairs, input_lang, output_lang, max_length=MAX_LENGTH)\n","\n","# Crear DataLoader\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","\n","#Modelo\n","model = TransformerPredictor(vocab_size, model_dim, num_classes, num_heads, num_layers, lr, warmup, max_iters, dropout, input_dropout)\n","logger = pl.loggers.CSVLogger(save_dir=\"logs\", name=f\"_exp_name\")"],"metadata":{"id":"pD-VbZ63RaK3","executionInfo":{"status":"ok","timestamp":1726016216727,"user_tz":300,"elapsed":1,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["\n","model.to(device)\n","trainer = pl.Trainer(max_epochs=num_epochs,\n","                    accelerator=\"auto\",\n","                    devices=\"auto\",\n","                     logger=logger\n","    )\n","trainer.fit(model, train_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":379,"referenced_widgets":["684253afdcf7474b8733f199a480fe39","0d6563c8e9c14d00a03be52d20cc6fc0","c96db3612100410494761125f140ec42","1d3d1e374f104bc0843c576447740654","f09fa39cb5e44fe7903fd31217a2f078","dfd8615503bb47709420aec14de7ecb9","3a5d160abe894998b14dd8c0279c5e4f","92ea2ce457584bb6b0eea146c814be5f","b95564e9186248f7b5dad5d9a583d70e","0536b75a440a482c8630177850ac6b14","5419785caa584018a69abe292a66ac42"]},"id":"3bosAaI7RaIv","executionInfo":{"status":"ok","timestamp":1726016872733,"user_tz":300,"elapsed":653948,"user":{"displayName":"fgony","userId":"12699453911390145003"}},"outputId":"18d0ce89-8238-4316-b9d6-cceb628709c4"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n","INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","INFO:pytorch_lightning.callbacks.model_summary:\n","  | Name                | Type               | Params | Mode \n","-------------------------------------------------------------------\n","0 | embedding           | Embedding          | 749 K  | train\n","1 | positional_encoding | PositionalEncoding | 0      | train\n","2 | transformer         | TransformerEncoder | 17.1 K | train\n","3 | output_net          | Sequential         | 773 K  | train\n","-------------------------------------------------------------------\n","1.5 M     Trainable params\n","0         Non-trainable params\n","1.5 M     Total params\n","6.160     Total estimated model params size (MB)\n","34        Modules in train mode\n","0         Modules in eval mode\n"]},{"output_type":"display_data","data":{"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"684253afdcf7474b8733f199a480fe39"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=30` reached.\n"]}]},{"cell_type":"code","source":["log_dir = trainer.logger.log_dir\n","\n","metrics = pd.read_csv(f\"{log_dir}/metrics.csv\")\n","\n","aggreg_metrics = []\n","agg_col = \"epoch\"\n","for i, dfg in metrics.groupby(agg_col):\n","      agg = dict(dfg.mean())\n","      agg[agg_col] = i\n","      aggreg_metrics.append(agg)\n","\n","df_metrics = pd.DataFrame(aggreg_metrics)"],"metadata":{"id":"hH1RVlc1u3gr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def translate_sentence(model, sentence, input_lang, output_lang, device):\n","    model.eval()\n","\n","    def tensorFromSentence(lang, sentence, device): # Added device as an argument\n","        indexes = [lang.word2index.get(word, 0) for word in sentence.split(' ')]\n","        indexes.append(EOS_token)  # Add EOS token as an integer\n","        return torch.tensor(indexes, dtype=torch.long, device=device).unsqueeze(0) # Use device here\n","\n","    # Pass device to tensorFromSentence\n","    input_tensor = tensorFromSentence(input_lang, sentence, device)\n","\n","    with torch.no_grad():\n","        output = model(input_tensor)\n","\n","        output = output.squeeze(0)\n","        _, topi = output.topk(1, dim=-1)\n","\n","        decoded_indices = topi.squeeze().tolist()\n","\n","    translated_words = []\n","    for index in decoded_indices:\n","        translated_words.append(output_lang.index2word.get(index, 'Unknown'))\n","\n","    return ' '.join(translated_words)"],"metadata":{"id":"e2UPGER0YjWz","executionInfo":{"status":"ok","timestamp":1726016943589,"user_tz":300,"elapsed":418,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model.to(device)\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mTh4zJz7oEB6","executionInfo":{"status":"ok","timestamp":1726016970417,"user_tz":300,"elapsed":414,"user":{"displayName":"fgony","userId":"12699453911390145003"}},"outputId":"3a0385d8-26dd-4d2d-9eaa-8c565ff4b44f"},"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":64}]},{"cell_type":"markdown","source":["# Comparación Attention Model"],"metadata":{"id":"lMBabWoaukmr"}},{"cell_type":"code","source":["input_sentence = 'he is my brother'\n","translated_sentence = translate_sentence(model, input_sentence, input_lang, output_lang, device)\n","print('Transformer:', translated_sentence)\n","print('Attention:', 'el es mi hermano mayor <EOS>')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K-yZFI0JjsFP","executionInfo":{"status":"ok","timestamp":1726019190412,"user_tz":300,"elapsed":1052,"user":{"displayName":"fgony","userId":"12699453911390145003"}},"outputId":"c0641a0b-9e27-414e-de9b-3cb8ae22a877"},"execution_count":108,"outputs":[{"output_type":"stream","name":"stdout","text":["Transformer: el es mi hermano EOS\n","Attention: el es mi hermano mayor <EOS>\n"]}]},{"cell_type":"code","source":["input_sentence = 'my car broke down this week'\n","translated_sentence = translate_sentence(model, input_sentence, input_lang, output_lang, device)\n","print('Transformer:', translated_sentence)\n","print('Attention:', 'mi coche me esta romper esta semana <EOS>')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l3VaJmrTYjUU","executionInfo":{"status":"ok","timestamp":1726019089280,"user_tz":300,"elapsed":1038,"user":{"displayName":"fgony","userId":"12699453911390145003"}},"outputId":"cbf98841-ead0-4b33-c53c-8a2a7aad6b22"},"execution_count":105,"outputs":[{"output_type":"stream","name":"stdout","text":["Transformer: mi coche el la la semana EOS\n","Attention: mi coche me esta romper esta semana <EOS>\n"]}]},{"cell_type":"markdown","source":["# Attention  Model"],"metadata":{"id":"EQKB8t7vuVHw"}},{"cell_type":"code","source":["class BahdanauAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(BahdanauAttention, self).__init__()\n","        self.Wa = nn.Linear(hidden_size, hidden_size)\n","        self.Ua = nn.Linear(hidden_size, hidden_size)\n","        self.Va = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, query, keys):\n","        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n","        scores = scores.squeeze(2).unsqueeze(1)\n","\n","        weights = F.softmax(scores, dim=-1)\n","        context = torch.bmm(weights, keys)\n","\n","        return context, weights"],"metadata":{"id":"fVmTPihhkyoZ","executionInfo":{"status":"ok","timestamp":1726017348297,"user_tz":300,"elapsed":1183,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["class AttnDecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n","        super(AttnDecoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.attention = BahdanauAttention(hidden_size)\n","        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n","        batch_size = encoder_outputs.size(0)\n","        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n","        decoder_hidden = encoder_hidden\n","        decoder_outputs = []\n","        attentions = []\n","\n","        for i in range(MAX_LENGTH):\n","            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n","                decoder_input, decoder_hidden, encoder_outputs\n","            )\n","            decoder_outputs.append(decoder_output)\n","            attentions.append(attn_weights)\n","\n","            if target_tensor is not None:\n","                # Teacher forcing: Feed the target as the next input\n","                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n","            else:\n","                # Without teacher forcing: use its own predictions as the next input\n","                _, topi = decoder_output.topk(1)\n","                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n","\n","        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n","        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n","        attentions = torch.cat(attentions, dim=1)\n","\n","        return decoder_outputs, decoder_hidden, attentions\n","\n","    def forward_step(self, input, hidden, encoder_outputs):\n","        embedded =  self.dropout(self.embedding(input))\n","\n","        query = hidden.permute(1, 0, 2)\n","        context, attn_weights = self.attention(query, encoder_outputs)\n","        input_gru = torch.cat((embedded, context), dim=2)\n","\n","        output, hidden = self.gru(input_gru, hidden)\n","        output = self.out(output)\n","\n","        return output, hidden, attn_weights"],"metadata":{"id":"KxCov8O8Qtrc","executionInfo":{"status":"ok","timestamp":1726017364078,"user_tz":300,"elapsed":411,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["def indexesFromSentence(lang, sentence):\n","    return [lang.word2index[word] for word in sentence.split(' ')]\n","\n","def tensorFromSentence(lang, sentence):\n","    indexes = indexesFromSentence(lang, sentence)\n","    indexes.append(EOS_token)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n","\n","def tensorsFromPair(pair):\n","    input_tensor = tensorFromSentence(input_lang, pair[0])\n","    target_tensor = tensorFromSentence(output_lang, pair[1])\n","    return (input_tensor, target_tensor)\n","\n","def get_dataloader(batch_size):\n","    input_lang, output_lang, pairs = prepareData('eng', 'spa', file)\n","\n","    n = len(pairs)\n","    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n","    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n","\n","    for idx, (inp, tgt) in enumerate(pairs):\n","        inp_ids = indexesFromSentence(input_lang, inp)\n","        tgt_ids = indexesFromSentence(output_lang, tgt)\n","        inp_ids.append(EOS_token)\n","        tgt_ids.append(EOS_token)\n","        input_ids[idx, :len(inp_ids)] = inp_ids\n","        target_ids[idx, :len(tgt_ids)] = tgt_ids\n","\n","    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n","                               torch.LongTensor(target_ids).to(device))\n","\n","    train_sampler = RandomSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","    return input_lang, output_lang, train_dataloader"],"metadata":{"id":"OWeQFOPnp0Ws","executionInfo":{"status":"ok","timestamp":1726017440802,"user_tz":300,"elapsed":1108,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","    def forward(self, input):\n","        embedded = self.dropout(self.embedding(input))\n","        output, hidden = self.gru(embedded)\n","        return output, hidden"],"metadata":{"id":"GgS26klRqIQ4","executionInfo":{"status":"ok","timestamp":1726017495435,"user_tz":300,"elapsed":1561,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":84,"outputs":[]},{"cell_type":"code","source":["class DecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size):\n","        super(DecoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n","        self.out = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n","        batch_size = encoder_outputs.size(0)\n","        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n","        decoder_hidden = encoder_hidden\n","        decoder_outputs = []\n","\n","        for i in range(MAX_LENGTH):\n","            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n","            decoder_outputs.append(decoder_output)\n","\n","            if target_tensor is not None:\n","                # Teacher forcing: Feed the target as the next input\n","                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n","            else:\n","                # Without teacher forcing: use its own predictions as the next input\n","                _, topi = decoder_output.topk(1)\n","                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n","\n","        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n","        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n","        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n","\n","    def forward_step(self, input, hidden):\n","        output = self.embedding(input)\n","        output = F.relu(output)\n","        output, hidden = self.gru(output, hidden)\n","        output = self.out(output)\n","        return output, hidden"],"metadata":{"id":"LqMZaTS9qL8H","executionInfo":{"status":"ok","timestamp":1726017489783,"user_tz":300,"elapsed":605,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler"],"metadata":{"id":"g8JFYhYOqTKo","executionInfo":{"status":"ok","timestamp":1726017560300,"user_tz":300,"elapsed":2248,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n","          decoder_optimizer, criterion):\n","\n","    total_loss = 0\n","    for data in dataloader:\n","        input_tensor, target_tensor = data\n","\n","        encoder_optimizer.zero_grad()\n","        decoder_optimizer.zero_grad()\n","\n","        encoder_outputs, encoder_hidden = encoder(input_tensor)\n","        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n","\n","        loss = criterion(\n","            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n","            target_tensor.view(-1)\n","        )\n","        loss.backward()\n","\n","        encoder_optimizer.step()\n","        decoder_optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    return total_loss / len(dataloader)"],"metadata":{"id":"gRQ63KYFq0p8","executionInfo":{"status":"ok","timestamp":1726017657135,"user_tz":300,"elapsed":1176,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":93,"outputs":[]},{"cell_type":"code","source":["def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n","               print_every=100, plot_every=100):\n","    #start = time.time()\n","    plot_losses = []\n","    print_loss_total = 0  # Reset every print_every\n","    plot_loss_total = 0  # Reset every plot_every\n","\n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n","    criterion = nn.NLLLoss()\n","\n","    for epoch in range(1, n_epochs + 1):\n","        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n","        print_loss_total += loss\n","        plot_loss_total += loss\n","\n","        if epoch % print_every == 0:\n","            print_loss_avg = print_loss_total / print_every\n","            print_loss_total = 0\n","            #print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n","            #                            epoch, epoch / n_epochs * 100, print_loss_avg))\n","\n","        if epoch % plot_every == 0:\n","            plot_loss_avg = plot_loss_total / plot_every\n","            plot_losses.append(plot_loss_avg)\n","            plot_loss_total = 0\n","\n"," #   showPlot(plot_losses)"],"metadata":{"id":"XW2eQN7Rqkkj","executionInfo":{"status":"ok","timestamp":1726017628640,"user_tz":300,"elapsed":1343,"user":{"displayName":"fgony","userId":"12699453911390145003"}}},"execution_count":91,"outputs":[]}]}